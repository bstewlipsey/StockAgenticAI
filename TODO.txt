# AGENTIC STOCK AI - COMPREHENSIVE TO-DO LIST (Trading Automation Focus)

**OVERARCHING GOAL:** Achieve a fully automatic, agentic crypto and stock trading AI that consistently identifies and executes actionable trades within your Gemini API quota, learns from its experiences, and operates reliably.

**CURRENT STATUS (Based on Latest Logs & Reports):**

*   All foundational setup and individual bot self-tests are verified.
*   Core system integration (minimal trading loop) is operational.
*   LLM memory (Task 3.8) and news sentiment integration (Task 3.9) appear to be partially functional for stock assets, but require explicit verification.
*   PortfolioBot and KnowledgeGraphBot are fully functional and pass their self-tests.
*   `HOLD_OVERRIDE` logic is active and pushing decisions.

**CRITICAL BLOCKERS REMAINING (PRIMARY FOCUS):**

*   **Automated Trade Execution Failure:** The system is not making trades automatically despite successful individual bot tests and internal "Order placed successfully" logs. This is the highest priority for resolution for both stock and crypto assets.
*   **Crypto LLM JSON Parsing Failure:** LLM output for crypto (ETH/USD) is valid JSON, but the system fails to parse it.
*   **Kraken Symbol Mismatch:** `BTC/USD` is incorrectly used for Kraken, which requires `XBT/USD`.
*   **Aggressive `TRADING_CYCLE_INTERVAL`:** Current setting (360s) is too low for Gemini API quota.

**ISSUES TO ADDRESS:**

*   Understanding and resolving the automatic trade execution and recording failure for all asset types (crypto and stock).
*   Verifying the correctness and utility of the logging logic against expected system flow for both crypto and stock trading.
*   Making all log output more human-readable while retaining arguments and returns.
*   Incorporating Alpaca data for immediate post-trade action confirmation and erroring.
*   Reliable LLM JSON parsing for all assets (crypto and stock).
*   Correct crypto data retrieval.
*   Sustainable Gemini API quota usage.
*   Refining AI "HOLD" bias for all asset types.
*   Thorough, progressive testing (paper to live) for both crypto and stock.
*   Continuous learning and optimization across all asset types.

**STRICT ARCHITECTURAL REQUIREMENTS (MANDATORY ADHERENCE):**

*   All configuration must be in `config_system.py` (system/API) and `config_trading.py` (strategy/bot).
*   All shared data structures must be in `data_structures.py`.
*   All bot files must remain in the project root and follow the naming convention `bot_<purpose>.py`.
*   All new configuration or data structure files must be placed in the project root.
*   All test files must be named `test_*.py` and placed in the project root.
*   Do not move or rename existing files unless explicitly required.
*   **DO NOT LEAVE PLACEHOLDER AREAS IN CODE OR CONFIGURATION.** If a placeholder is necessary to implement another section, include a new, specific task in this to-do list to implement that placeholder's functionality.
*   No method or class in production may return, print, or log a placeholder, stub, or dummy value (such as 'Actual LLM response', 'Not implemented', or similar) except when explicitly in a test or mock context. Any such case must be strictly gated by a test/mock flag, and any violation must raise an exception. This rule applies to all code, not just LLM calls, and is mandatory to prevent silent failures, incomplete integration, or misleading outputs anywhere in the system.

**CRITICAL DIRECTIVES - READ CAREFULLY AND ADHERE STRICTLY:**

*   **SEQUENTIAL EXECUTION (NO SKIPPING):** Process the to-do list section by section, and task by task within each section, in the exact order presented. Do NOT jump ahead.
*   **DO NOT ASSUME COMPLETION:** For every single task, you MUST NOT assume it is already completed, even if its **STATUS** is marked as "✅ Completed". Your first action for any "Completed" task should be to verify its completion by reviewing code, running associated tests (`selftest()`, `pytest`), and checking logs/configurations. If verification fails, report and fix.
*   **THOROUGH INDIVIDUAL TESTING:** For all development tasks, implement/verify and run `selftest()` methods for affected bots. Run relevant `test_*.py` files to prevent regressions.
*   **LOGGING AND OUTPUT:** Minimize verbose console/log output. Ensure full prompts are logged only at `DEBUG` level or when `LOG_FULL_PROMPT` is enabled. Ensure all arguments and returns are logged in a human-readable format, even if large, using summarization or truncation where appropriate (see Task 3.2).
*   **REPORTING:** After attempting each task, clearly state the task number/title, actions taken, outcome, and any code changes. If a task is marked "Completed," report the result of your verification.

---
## Section 0: Project Setup & Initial Environment (Foundation)

### 0.1. Verify Project Structure and File Presence
*   **STATUS:** ✅ Completed. All required files are present.

### 0.2. Install Python Dependencies
*   **STATUS:** ✅ Completed. All dependencies installed successfully.

### 0.3. Configure `.env` and API Keys
*   **STATUS:** ✅ Completed. `.env` file contains all required API keys and settings.

### 0.4. Initial Review of `config_system.py` and `config_trading.py`
*   **STATUS:** ✅ Completed. Config files reviewed; no immediate duplicates found.

### 0.5. Initial Review of `data_structures.py`
*   **STATUS:** ✅ Completed. Core dataclasses are present and consistent.

---
## Section 1: Individual Bot Self-Testing & Code Redundancy Elimination

**General Instruction for Bots:** For each `bot_*.py` file, ensure a robust `selftest()` method is implemented within the bot's class. This method should: initialize the bot (use dummy/mocked APIs), perform a small, representative operation, include specific functionality assertions (using `assert`), print clear success/failure messages, and be callable directly from the command line using an `if __name__ == "__main__":` block. Ensure its internal placeholders are addressed by the specific `selftest` implementations.

### 1.1. `bot_ai.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. LLM call and parsing logic verified.

### 1.2. `bot_stock.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Data retrieval and buy/sell JSON injection logic verified.

### 1.3. `bot_crypto.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Data retrieval, error handling, and buy/sell JSON injection logic verified.

### 1.4. `bot_news_retriever.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. News retrieval and embedding logic verified.

### 1.5. `bot_indicators.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Indicator calculation logic verified.

### 1.6. `bot_asset_screener.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Asset screening and fallback logic verified.

### 1.7. `bot_decision_maker.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Decision logic and `min_confidence` application verified.

### 1.8. `bot_position_sizer.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Position sizing and edge case logic verified.

### 1.9. `bot_risk_manager.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Risk metrics and edge case logic verified.

### 1.10. `bot_trade_executor.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Trade execution and account info logic verified.

### 1.11. `bot_portfolio.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Portfolio state and metrics logic verified.

### 1.12. `bot_database.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. DB CRUD and retrieval logic verified.

### 1.13. `bot_reflection.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Insight generation and retrieval logic verified.

### 1.14. `bot_visualizer.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Display and output logic verified.

### 1.15. `bot_report.py` Self-Test & Review
*   **STATUS:** ✅ Completed. `selftest()` implemented and passes. Report file creation and content logic verified.

### 1.16. Review `main.py` and `OrchestratorBot`
*   **STATUS:** ✅ Completed. `main.py` and `OrchestratorBot` reviewed. Orchestration, data flow, and modularity are clear.

### 1.17. Review `test_*.py` Files
*   **STATUS:** ✅ Completed. All test files exist, have valid test structures, and use `pytest`. All tests pass.

### 1.18. Consolidate and Eliminate Redundant Code/Configuration
*   **STATUS:** ✅ Completed. Duplicates removed, config centralized, all `selftests` pass.

---
## Section 2: Core System Integration & Initial AI Activation

### 2.1. Integrate Bots for Minimal Trading Loop
*   **STATUS:** ✅ Completed. `main.py` and `OrchestratorBot` successfully run the minimal trading loop.

### 2.2. Re-Address AI "HOLD" Issue (Prompt Tuning)
*   **STATUS:** ✅ Completed. AI Prompt Templates adjusted to demand BUY/SELL signals, few-shot examples added, `MIN_CONFIDENCE` lowered to 0.3.

### 2.2a. Investigate and Address AI 'HOLD' Reasoning Patterns (Prompt Refinement)
*   **STATUS:** ✅ Completed. Explicit fallback instructions added to prompt templates; HOLD only allowed if data missing or indicators ambiguous.

### 2.2b. LLM Summary Missing in Shutdown Report
*   **STATUS:** ✅ Completed. `bot_report.py` updated to guarantee LLM summary with robust error handling.

### 2.3. Paper Trading Test & HOLD Debugging (Initial)
*   **STATUS:** ✅ Completed. `TEST_MODE_ENABLED = True`, `TRADING_CYCLE_INTERVAL = 360`, enhanced logging, forced assets for test.

### 2.4. Implement Gemini API Response Logging (JSON Format)
*   **STATUS:** ✅ Completed. Raw LLM responses and JSON parsing attempts logged at `INFO`/`DEBUG` level.

### 2.5. Optimize Console/Log Output for Clarity
*   **STATUS:** ✅ Completed. Full prompts logged at `DEBUG` or with `LOG_FULL_PROMPT` flag.

### 2.6. Address Persistent and Justified 'HOLD' Outputs (Override Logic)
*   **STATUS:** ✅ Completed. Post-processing override logic implemented in `bot_decision_maker.py` to force BUY/SELL if indicators are present and not contradictory.

### 2.7. Expand and Automate Testing for 'HOLD' Override and Prompt Changes
*   **STATUS:** ✅ Completed. Unit, regression, and integration tests cover override scenarios.

---
## Section 3: Critical Debugging & Core Functionality (PRIMARY FOCUS)

### 3.1. PRIMARY FOCUS: Understand and Resolve Automated Trade Execution Failure
*   **PROBLEM:** The system is not making trades automatically in `main.py` runs, even though `TradeExecutorBot` reports "Order placed successfully" and individual bot tests pass.
*   **ACTION:**
    *   **Log Flow Analysis:** Meticulously analyze `trading.log` (especially at `DEBUG` level) from a full `main.py` run where trades should occur but don't.
    *   Does the log match the expected flow? Trace the entire trade lifecycle: asset screening -> market data/news retrieval -> AI analysis -> decision making -> position sizing -> risk management -> trade execution -> portfolio recording -> database saving.
    *   Identify the exact point of failure: Based on the logs, pinpoint where the divergence from the expected flow occurs, or where data that should lead to a trade is lost/transformed incorrectly.
    *   **Post-Trade Alpaca Data Confirmation & Erroring:**
        *   Immediately after any call to `TradeExecutorBot.execute_trade`, regardless of its internal success message, make an explicit, synchronous API call to Alpaca to verify the order status (e.g., `get_order_by_client_order_id` or `get_position`).
        *   Compare the actual Alpaca response with the system's internal expectation for the trade.
        *   Log discrepancies: Log `WARNING` or `ERROR` messages for any mismatches (e.g., order not found on Alpaca, order status unexpected, partial fill different from expected).
        *   Implement critical error handling: If a trade is internally marked successful but is not confirmed on Alpaca, trigger a `CRITICAL` log and, in `TEST_MODE_ENABLED = True`, consider raising an `Exception` or halting the cycle to force immediate investigation. In live mode, consider an alert or retry mechanism.
    *   **Trace Internal Recording:** Confirm the calls to `PortfolioBot.add_or_update_position` and `DatabaseBot.save_trade_outcome` are receiving the correct data and are actually persisting the trades after external Alpaca confirmation (or mock confirmation in test mode).
*   **VERIFICATION:** Run `main.py` in `TEST_MODE_ENABLED = True`. Confirm that `PortfolioBot.get_open_positions()` and `DatabaseBot.get_trade_history()` actually reflect the executed (mocked) trades, and that the new Alpaca confirmation logic identifies and logs discrepancies or errors if trades are not confirmed externally. Logs should clearly show the entire lifecycle without silent failures.

### 3.2. Simplify Log Output for Human Readability (Arguments & Returns)
*   **PROBLEM:** Logs contain verbose raw data (e.g., full embedding vectors, large JSON structures, long argument lists) making them difficult to quickly read and analyze.
*   **ACTION:**
    *   Review all `[ENTRY] args:` and `[EXIT] returned:` log statements across all bots.
    *   For large data structures (lists of numbers like embeddings, extensive dictionaries/JSON, long strings):
        *   **Summarize:** Log only key fields, a truncated version (e.g., first 5-10 elements of an array), or a hash/checksum for identification.
        *   **Conditional Verbosity:** Use `DEBUG` level for full outputs that are rarely needed.
    *   Ensure any logged arguments and returns are concise and meaningful, providing just enough context without overwhelming the reader.
    *   Revisit `bot_ai.AIBot`'s `generate_embedding` method's exit log as the primary example to fix, but apply this principle systematically.
*   **VERIFICATION:** Run `main.py` in `TEST_MODE_ENABLED = True`. Review `trading.log` to confirm that while arguments and returns are still present, their format is significantly more compact and human-readable.

### 3.3. FIX: Crypto LLM JSON Parsing Error (`Expecting value: line 1 column 1`)
*   **PROBLEM:** LLM output for crypto (ETH/USD) is valid JSON, but the system fails to parse it.
*   **ACTION:** In `bot_crypto.py` (or the parsing utility), add a `try-except json.JSONDecodeError as e:` block. Inside the `except` block, print `repr(raw_llm_response_string)` to reveal any hidden characters or truncation. Debug and fix the parsing logic.
*   **VERIFICATION:** Run `main.py` in test mode. Confirm no `[WARNING] bot_crypto: [AI_JSON] Could not parse LLM response` errors for crypto assets.

### 3.4. FIX: Kraken `XBT/USD` Symbol Mismatch
*   **PROBLEM:** `bot_backtester` and `bot_crypto` fail to load `BTC/USD` data from Kraken because it expects `XBT/USD`.
*   **ACTION:** Locate where `BTC/USD` is used as a symbol for Kraken queries (likely in `config_trading.py` or `bot_crypto.py`) and change it to `XBT/USD` specifically for the Kraken exchange.
*   **VERIFICATION:** Run `main.py` in test mode. Confirm no `Error loading crypto data for BTC/USD from Kraken: kraken does not have market symbol XBT/USD` errors.

### 3.5. ADJUST: Gemini API Quota & `TRADING_CYCLE_INTERVAL`
*   **PROBLEM:** `TRADING_CYCLE_INTERVAL` (360s) is too aggressive for 500 RPD quota, potentially leading to 429 errors.
*   **ACTION:** In `config_system.py`, set `TRADING_CYCLE_INTERVAL` to at least 2000 seconds (approx. 33-34 minutes) or higher for a safe buffer, especially with multiple assets.
*   **VERIFICATION:** Run `main.py` for a few cycles. Monitor `trading.log` for any 429 errors or `RuntimeError` related to `GeminiKeyManagerBot` running out of keys.

### 3.6. VERIFY: AI "HOLD" Bias in Live Operation (Post-Fixes)
*   **PROBLEM:** AI might still show a bias towards "HOLD" even after prompt tuning.
*   **ACTION:** After fixing the trade execution and logging issues, run `main.py` in paper trading mode. Analyze `trading.log` for the distribution of BUY/SELL/HOLD decisions. Focus on the AI's reasoning for "HOLD" to ensure it's genuinely justified.
*   **VERIFICATION:** Observe a healthy mix of actionable BUY/SELL decisions in logs, not just "HOLD."

### 3.7. VERIFY: "zero or negative quantity/entry price" Warnings
*   **PROBLEM:** Previous logs showed warnings like `Position CMCSA rejected: zero or negative quantity/entry price`.
*   **ACTION:** After fixing the trade execution and logging issues, run tests targeting assets that previously caused these warnings. Trace quantity and entry price values from decision-making to position sizing.
*   **VERIFICATION:** Ensure these specific warnings no longer appear in logs.

### 3.8. VERIFY: Implement LLM Answer Memory for Decision-Making
*   **PROBLEM:** Logs show `[LLM_MEMORY] Found similar past LLM answer... Reusing.`, but explicit verification of full context storage and reuse logic is needed.
*   **ACTION:** Confirm `ReflectionBot` stores full LLM context (market data, news, prompt, raw LLM response) in `DatabaseBot`. Run `main.py` in test mode and confirm `trading.log` shows instances of decisions being reused from memory, leading to fewer direct LLM calls for repeated contexts.
*   **VERIFICATION:** Observe `[LLM_MEMORY] Reusing` messages in logs and a reduction in total LLM calls for recurring scenarios.

### 3.9. VERIFY: Integrate News Sentiment into LLM Answer Memory and Decision Logic
*   **PROBLEM:** Logs indicate `news similarity=1.00` for reused answers, but explicit verification of news sentiment's role in context and correlation is needed.
*   **ACTION:** Verify `NewsRetrieverBot` correctly retrieves and stores news sentiment. Confirm LLM answer memory (Task 3.8) incorporates news sentiment when deciding whether to reuse a past answer.
*   **VERIFICATION:** Check `ReflectionBot` insights for correlations between news context and trade outcomes.

### 3.10. VERIFIED: Implement Alpaca Order History Cross-Check within System (Periodic)
*   **STATUS:** ✅ Completed (This task is now for periodic background checks, complementing the immediate post-trade verification in Task 3.1). `TradeExecutorBot` or `PortfolioBot` periodically fetches Alpaca order history and compares with internal records.
*   **Re-verification required:** After new Alpaca confirmation logic in Task 3.1 is implemented, re-verify this periodic check still functions correctly and consistently.

### 3.11. VERIFIED: Experiment with Different LLM Prompt Structures or Models
*   **STATUS:** ✅ Completed. Prompts refined, and the system is set up for experimentation if needed.

---
## Section 4: Advanced Functionality & Enhancements

### 4.1. VERIFIED: Integrate `ReflectionBot` and `RiskManager` (Runtime Verification)
*   **STATUS:** ✅ Completed. `ReflectionBot` insights saved to DB, `RiskManagerBot` reports active.

### 4.2. VERIFIED: Implement Comprehensive Bot Status Reporting Feature
*   **STATUS:** ✅ Completed. `ReportBot` health summary implemented.

### 4.3. VERIFIED: Research and Implement Knowledge Graph for AI Learning (Advanced)
*   **STATUS:** ✅ Completed. `KnowledgeGraphBot` implemented with NetworkX for decisions/outcomes.

---
## Section 5: Documentation & Automation

### 5.1. Update `README.md` and Documentation
*   **STATUS:** ⏳ To Do.
*   **ACTION:** Review and update the "Project Status" section to accurately reflect current capabilities and limitations, especially after the logging overhaul and trade execution fixes. Update instructions for setup, configuration (including new flags like `SELFTEST_LIVE_API_CALLS_ENABLED`, `LOG_FULL_PROMPT`, `FILE_LOG_LEVEL`), and operation. Ensure `VisualizerBot.display_interactive_log_viewer()` usage is current, if applicable. Add a prominent note about the new logging format and the human-readability improvements.
*   **VERIFICATION:** `README.md` is accurate, up-to-date, and provides clear guidance for users/developers.

### 5.2. Automate Test Runs and Log Monitoring
*   **STATUS:** ⏳ To Do.
*   **ACTION:** Execute the `run_tests_and_log_check.ps1` script. Confirm it runs all `pytest` tests successfully. Verify it performs meaningful checks on `trading.log` for critical errors (e.g., "ERROR", "CRITICAL", "Traceback", "LLM_JSON_PARSE_FAIL", and now "Trade Confirmation Mismatch" or similar new errors from Task 3.1). Update the script if new critical log patterns need to be monitored.
*   **VERIFICATION:** Automation script runs all tests, effectively checks logs for critical issues, and reports a clear overall status.

---
## Section 6: Deployment & Initial Live Trading (User's Progressive Testing Plan)

### 6.1. PREPARE: Final Checks Before Extended Testing
*   **STATUS:** ⏳ To Do
*   **ACTION:** Ensure all tasks in Section 3 are marked as FIXED or VERIFIED. Confirm `TRADING_CYCLE_INTERVAL` is set appropriately for quota (Task 3.5). Review `RISK_TOLERANCE` in `config_trading.py` for desired aggressiveness.
*   **VERIFICATION:** All preceding critical tasks are confirmed complete and verified.

### 6.2. TEST: 8-Hour Paper Trading Session
*   **STATUS:** ⏳ To Do
*   **ACTION:** Set `TEST_MODE_ENABLED = True` in `config_system.py`. Run `python main.py` for 8 continuous hours.
*   **VERIFICATION:**
    *   **Crucially:** Review `trading.log` to confirm that automated trades are now being executed and correctly recorded internally.
    *   Confirm logs match expected flow: Trace multiple trade lifecycles in the logs to ensure they accurately reflect the system's intended behavior from decision to execution to recording and Alpaca confirmation.
    *   Review `trading.log` for any errors (429s, zero/negative quantity, JSON parsing errors, trade confirmation mismatches from Task 3.1).
    *   Check ratio of BUY/SELL/HOLD decisions.
    *   Verify `ReflectionBot` is saving insights. Review generated report.

### 6.3. TEST: 24-Hour (1-Day) Paper Trading Session
*   **STATUS:** ⏳ To Do
*   **ACTION:** Continue running `python main.py` for a full 24 hours (or restart after 8-hour test).
*   **VERIFICATION:** Perform all checks from the 8-hour test. Monitor API usage over 24 hours to ensure it stays within the 500 RPD limit. Assess if AI decisions show patterns or learning over time.

### 6.4. DEPLOY: 1-Week Live Trading Session (Controlled Risk)
*   **STATUS:** ⏳ To Do
*   **ACTION:** Only proceed if all paper trading tests pass consistently. Set `ENABLE_TRADING_BOT = True` in `config_system.py`. Start with a very small amount of actual capital. Run `python main.py` live for one week.
*   **VERIFICATION:** Monitor your Alpaca account directly for executed trades. Cross-check Alpaca order history with `trading.log` (Task 3.10) and verify the immediate post-trade confirmation (Task 3.1). Track portfolio performance (realized P&L). Continue monitoring API usage and logs for errors. Generate daily reports.

### 6.5. DEPLOY: 1-Month Live Trading Session
*   **STATUS:** ⏳ To Do
*   **ACTION:** Continue running `python main.py` live for one month.
*   **VERIFICATION:** Perform all checks from the 1-week test. Analyze long-term performance trends (win rate, profit factor, drawdown). Evaluate if AI learning from reflection is leading to improved decision-making. Look for new edge cases or errors.

---
## Section 7: Continuous Improvement & Maintenance

### 7.1. Add Any New Errors, Warnings, or Improvement Ideas
*   **STATUS:** ⏳ To Do
*   **ACTION:** Promptly add them to this TODO list and address them as they arise during extended testing or live operation.
*   **VERIFICATION:** N/A (Ongoing process).

### 7.2. Conduct Extended Paper Trading/Simulation (Ongoing)
*   **STATUS:** ⏳ To Do
*   **ACTION:** Continue running `main.py` in paper trading mode periodically for ongoing validation of end-to-end functionality and strategy performance without financial risk.
*   **VERIFICATION:** Continuously monitor stability, performance trends, API usage, and AI learning through logs and reports.

### 7.3. Ongoing AI Learning & Reflection Monitoring
*   **STATUS:** ⏳ To Do
*   **ACTION:** Regularly review `ReflectionBot` insights to understand AI learning patterns and identify areas for prompt or strategy refinement.
*   **VERIFICATION:** Insights are clear, relevant, and provide actionable feedback on AI decisions.

### 7.4. Strategy Optimization & Backtesting
*   **STATUS:** ⏳ To Do
*   **ACTION:** Use `BacktesterBot` to run parameter sweeps and validate strategy robustness based on historical data. Refine `config_trading.py` parameters.
*   **VERIFICATION:** Backtest results show consistent positive performance across different market conditions.
